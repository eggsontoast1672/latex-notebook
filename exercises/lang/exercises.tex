\documentclass[12pt, titlepage]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\usepackage{parskip}

\title{Solutions to Lang Exercises}
\author{Paul Zupan}
\date{Last edited \today}

\newenvironment{exercise}{\paragraph{Exercise.}}{}
\newenvironment{solution}{\paragraph{Solution.}}{}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\maketitle

\section{Vector Spaces}

\subsection{Definitions}

\begin{exercise}
  Let $V$ be a vector space. Using the properties \textbf{VS 1} through
  \textbf{VS 8}, show that if $c$ is a number, then $cO = O$.
\end{exercise}

\begin{solution}
  To show that $cO = O$, it is sufficient to show that $cO + v = v$ for all
  $v$.

  \[
    \begin{aligned}
      cO + v
        & = cO + cv - (c - 1)v \\
        & = c(O + v) - (c - 1)v \\
        & = cv - (c - 1)v \\
        & = v \\
    \end{aligned}
  \]
\end{solution}

\begin{exercise}
  In the vector space of functions, what is the function satisfying the
  condition \textbf{VS 2}?
\end{exercise}

\begin{solution}
  In the vector space of functions, the zero vector is the constant zero
  function $O : A \to \R$ with $O(a) = 0$ for all $a$.

  \[
    \begin{aligned}
      (O + f)(a)
        & = O(a) + f(a) \\
        & = 0 + f(a)    \\
        & = f(a)        \\
        & = f(a) + 0    \\
        & = f(a) + O(a) \\
        & = (f + O)(a)  \\
    \end{aligned}
  \]
\end{solution}

\subsection{Bases}

\begin{exercise}
  Show that the following vectors are linearly independent (over $\C$ or $\R$).

  \begin{multicols}{2}
    \begin{enumerate}[(a), noitemsep]
      \item $(1, 1, 1)$ and $(0, 1, -2)$
      \item $(1, 0)$ and $(1, 1)$
      \item $(-1, 1, 0)$ and $(0, 1, 2)$
      \item $(2, -1)$ and $(1, 0)$
      \item $(\pi, 0)$ and $(0, 1)$
      \item $(1, 2)$ and $(1, 3)$
      \item $(1, 1, 0)$, $(1, 1, 1)$, and $(0, 1, -1)$
      \item $(0, 1, 1)$, $(0, 2, 1)$, and $(1, 5, 3)$
    \end{enumerate}
  \end{multicols}
\end{exercise}

\begin{solution}
  \begin{enumerate}[(a)]
    \item Let $a$ and $b$ be real numbers. We assume that $a(-1, 1, 0) + b(0,
      1, 2) = 0$. This gives us three linear equations:

      \[
        \begin{aligned}
          -a    & = 0 \\
          a + b & = 0 \\
          2b    & = 0 \\
        \end{aligned}
      \]

      It is immediate that $a = 0$ and $b = 0$. Thus, the two vectors are
      linearly independent.
  \end{enumerate}
\end{solution}

\begin{exercise}
  Express the given vector $X$ as a linear combination of the given vectors $A,
  B$, and find the coordinates of $X$ with respect to $A, B$.

  \begin{enumerate}[(a), noitemsep]
    \item $X = (1, 0)$, $A = (1, 1)$, $B = (0, 1)$
    \item $X = (2, 1)$, $A = (1, -1)$, $B = (1, 1)$
    \item $X = (1, 1)$, $A = (2, 1)$, $B = (-1, 0)$
    \item $X = (4, 3)$, $A = (2, 1)$, $B = (-1, 0)$
  \end{enumerate}
\end{exercise}

\begin{solution}
  \textit{Part (b).} This example is not so trivial. However, we can solve this
  problem by creating the matrix whose first two columns are $A$ and $B$, and
  whose last column is $X$. Then, we can perform Gauss Jordan elimination on
  this matrix to find the desired linear combination.

  \[
    \begin{bmatrix}
      1  & 1 & 2 \\
      -1 & 1 & 1 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 1 & 2 \\
      0 & 2 & 3 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 1 & 2   \\
      0 & 1 & 3/2 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 0 & 1/2 \\
      0 & 1 & 3/2 \\
    \end{bmatrix}
  \]

  Thus, we can write $X = 0.5A + 1.5B$. The coordinates of $X$
  with respect to $A, B$ are just the coefficients of $A$ and $B$ in this
  representation, which are $(0.5, 1.5)$.
\end{solution}

\begin{exercise}
  Consider the vector space of all functions of a variable $t$. Show that the
  following pairs of functions are linearly independent.

  \begin{multicols}{3}
    \begin{enumerate}[(a), noitemsep]
      \item 1, $t$
      \item $t$, $t^2$
      \item $t$, $t^4$
      \item $e^t$, $t$
      \item $te^t$, $e^{2t}$
      \item $\sin t$, $\cos t$
      \item $t$, $\sin t$
      \item $\sin t$, $\sin 2t$
      \item $\cos t$, $\cos 3t$
    \end{enumerate}
  \end{multicols}
\end{exercise}

\begin{solution}
  \textit{Part (f).} We aim to show that if $a\sin t + b\cos t = 0$ for all
  $t$, then $a = 0$ and $b = 0$. Taking $t = 0$ in this equation gives $b = 0$,
  and taking $t = \pi/2$ gives $a = 0$. Thus, the functions are linearly
  independent.
\end{solution}

\begin{exercise}
  Consider the vector space of functions defined for $t > 0$. Show that the
  following pairs of functons are linearly independent.

  \begin{enumerate}[(a), noitemsep]
    \item $t$, $1 / t$
    \item $e^t$, $\log t$
  \end{enumerate}
\end{exercise}

\begin{solution}
  \textit{Part (a).} We aim to show that if $at + b / t = 0$ for all $t$, then
  $a = 0$ and $b = 0$. If we take $t = 1$, then we have $a + b = 0$. If we take
  $t = 2$, we get another equation $2a + b/2 = 0$. We can solve this system for
  $a$ and $b$ using Gauss Jordan elimination:

  \[
    \begin{bmatrix}
      1 & 1   \\
      2 & 1/2 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 1    \\
      0 & -3/2 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 1 \\
      0 & 1 \\
    \end{bmatrix}
    \sim \begin{bmatrix}
      1 & 0 \\
      0 & 1 \\
    \end{bmatrix}
  \]

  We omit the last column of zeros since it is redundant. The system is
  consistent, and we see that the unique solution is $a = 0$ and $b = 0$.
\end{solution}

\begin{exercise}
  Let $A_1, \dots, A_r$ be vectors in $\R^n$ and assume that they are mutually
  perpendicular (i.e. any two of them are perpendicular), and that none of them
  is equal to $O$. Prove that they are linearly independent.
\end{exercise}

\begin{solution}
  Suppose that $c_1A_1 + \cdots + c_rA_r = O$. Performing some algebra:

  \[
    \begin{aligned}
      c_1A_1 + \cdots + c_rA_r                     & = O           \\
      A_i \cdot (c_1A_1 + \cdots + c_rA_r)         & = A_i \cdot O \\
      c_1A_i \cdot A_1 + \cdots + c_rA_i \cdot A_r & = 0           \\
      c_iA_i^2                                     & = 0           \\
    \end{aligned}
  \]

  Since each pair of distinct vectors are orthogonal, the dot product leaves
  only $c_iA_i^2$ remaining on the left. Because of the positive definite
  property of the dot product, $A_i^2$ cannot be zero since $A_i$ is not zero
  by assumption. Thus, $c_i$ must be zero.
\end{solution}

\subsection{Sums and Direct Sums}

\begin{exercise}
  Let $V = \R^2$, and let $W$ be the subspace generated by $(2, 1)$. Let $U$ be
  the subspace generated by $(0, 1)$. Show that $V$ is the direct sum of $W$
  and $U$. If $U'$ is the subspace generated by $(1, 1)$, show that $V$ is also
  the direct sum of $W$ and $U'$.
\end{exercise}

\begin{solution}
  To show that $V = W \oplus U$, we need to show that any vector in $V$ can be
  written as a unique sum of a vector from $W$ and a vector from $U$. This is
  logically equivalent to showing that $(2, 1)$ and $(0, 1)$ form a basis for
  $V$.
\end{solution}

\end{document}
